{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<azure.search.documents.indexes._search_index_client.SearchIndexClient object at 0x71a4dc639950>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes.models import *\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "from openai import AzureOpenAI\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "import glob\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "# 開発環境・本番環境でも同じ認証方式を使用するため、DefaultAzureCredentialを用いて認証情報を取得する。\n",
    "azure_credential = DefaultAzureCredential()\n",
    "token_provider = get_bearer_token_provider(\n",
    "    DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\"\n",
    ")\n",
    "\n",
    "# 環境変数からAzure AI Search、Azure OpenAI、Azure Document Intelligenceのエンドポイントを取得する\n",
    "search_service_endpoint = os.environ[\"SEARCH_SERVICE_ENDPOINT\"]\n",
    "aoai_endpoint = os.environ[\"AOAI_ENDPOINT\"]\n",
    "aoai_api_version = os.environ[\"AOAI_API_VERSION\"]\n",
    "document_intelligence_endpoint = os.environ[\"DOCUMENT_INTELLIGENCE_ENDPOINT\"]\n",
    "\n",
    "# Jupyter Notebook環境で手動で引数を設定\n",
    "class Args:\n",
    "    docs = \"data\"  # インデックス対象のファイルが格納されているディレクトリを指定\n",
    "    chunksize = \"1000\"  # テキストを分割する際のサイズを指定\n",
    "    overlap = \"200\"  # テキストを分割する際のオーバーラップサイズを指定\n",
    "    remove = False  # インデックスを削除するかどうかを指定\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# テキストを分割する際の区切り文字を指定する\n",
    "separator = [\"\\n\\n\", \"\\n\", \"。\", \"、\", \" \", \"\"]\n",
    "\n",
    "def create_index():\n",
    "    \"\"\"\n",
    "    Azure AI Searchのインデックスを作成する\n",
    "    \"\"\"\n",
    "    client = SearchIndexClient(search_service_endpoint, azure_credential)\n",
    "    print(client)\n",
    "    name = \"docs\"\n",
    "\n",
    "    # すでにインデックスが作成済みである場合には何もしない\n",
    "    if 'docs' in client.list_index_names():\n",
    "        print(\"すでにインデックスが作成済みです\")\n",
    "        return\n",
    "\n",
    "    # インデックスのフィールドを定義する\n",
    "    fields = [\n",
    "        SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "        SearchableField(name=\"content\", type=\"Edm.String\", analyzer_name=\"ja.microsoft\"),\n",
    "        SearchField(name=\"contentVector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                searchable=True, vector_search_dimensions=1536, vector_search_profile_name=\"myHnswProfile\")\n",
    "    ]\n",
    "\n",
    "    # セマンティック検索のための定義を行う\n",
    "    semantic_settings = SemanticSearch(\n",
    "        configurations=[\n",
    "            SemanticConfiguration(\n",
    "                name=\"default\",\n",
    "                prioritized_fields=SemanticPrioritizedFields(\n",
    "                    title_field=None,\n",
    "                    content_fields=[\n",
    "                        SemanticField(field_name=\"content\")\n",
    "                    ],\n",
    "                ),\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # ベクトル検索のための定義を行う\n",
    "    vector_search = VectorSearch(\n",
    "        algorithms=[\n",
    "            HnswAlgorithmConfiguration(\n",
    "                name=\"myHnsw\"\n",
    "            )\n",
    "        ],\n",
    "        profiles=[\n",
    "            VectorSearchProfile(\n",
    "                name=\"myHnswProfile\",\n",
    "                algorithm_configuration_name=\"myHnsw\",\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # インデックスを作成する\n",
    "    index = SearchIndex(name=name, fields=fields, vector_search=vector_search, semantic_search=semantic_settings)\n",
    "    client.create_index(index)\n",
    "\n",
    "def delete_index():\n",
    "    \"\"\"\n",
    "    Azure AI Searchのインデックスを削除する\n",
    "    \"\"\"\n",
    "    client = SearchIndexClient(search_service_endpoint, azure_credential)\n",
    "    client.delete_index('docs')\n",
    "\n",
    "def index_docs(chunks: list):\n",
    "    \"\"\"\n",
    "    ドキュメントをAzure AI Searchにインデックスする\n",
    "    \"\"\"\n",
    "    # Azure AI SearchのAPIに接続するためのクライアントを生成する\n",
    "    searchClient = SearchClient(search_service_endpoint, \"docs\", azure_credential)\n",
    "\n",
    "    # Azure OpenAIのAPIに接続するためのクライアントを生成する\n",
    "    openAIClient = AzureOpenAI(azure_endpoint=aoai_endpoint, azure_ad_token_provider=token_provider, api_version = aoai_api_version)\n",
    "\n",
    "    # チャンク化されたテキストとそのテキストのベクトルをAzure AI Searchにアップロードする\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"{i+1}個目のチャンクを処理中...\")\n",
    "        response = openAIClient.embeddings.create(\n",
    "            input = chunk,\n",
    "            model = \"text-embedding-ada-002-deploy\"\n",
    "        )\n",
    "\n",
    "        # チャンク化されたテキストとそのテキストのベクトルをAzure AI Searchにアップロードする\n",
    "        document = {\"id\": str(i), \"content\": chunk, \"contentVector\": response.data[0].embedding}\n",
    "        searchClient.upload_documents([document])\n",
    "\n",
    "def create_chunk(content: str, separator: str, chunk_size: int = 512, overlap: int = 0):\n",
    "    \"\"\"\n",
    "    テキストを指定したサイズで分割する\n",
    "    \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_overlap=overlap, chunk_size=chunk_size, separators=separator)\n",
    "    chunks = splitter.split_text(content)\n",
    "    return chunks\n",
    "\n",
    "def extract_text_from_docs(filepath):\n",
    "    \"\"\"\n",
    "    ドキュメントからテキストを抽出する\n",
    "    \"\"\"\n",
    "    # Azure Document IntelligenceのAPIに接続するためのクライアントを生成する\n",
    "    form_recognizer_client = DocumentAnalysisClient(endpoint=document_intelligence_endpoint, credential=azure_credential)\n",
    "\n",
    "    # ドキュメントを読み込んで、Azure Document IntelligenceのAPIを呼び出して、テキストを抽出する\n",
    "    print(f\"{filepath}内のテキストを抽出中...\")\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        poller = form_recognizer_client.begin_analyze_document(\"prebuilt-layout\", document = f)\n",
    "    form_recognizer_results = poller.result()\n",
    "\n",
    "    # ドキュメントのテキストを抽出する\n",
    "    # ドキュメントのテキストは、ページごとに分割されているので、それを結合して返す\n",
    "    text = \"\"\n",
    "    for page in form_recognizer_results.pages:\n",
    "        for line in page.lines:\n",
    "            text += line.content\n",
    "    return text\n",
    "\n",
    "# Jupyter Notebook環境でのスクリプト実行部分\n",
    "if args.remove:\n",
    "    # 引数に--removeが指定されている場合には、インデックスを削除する\n",
    "    delete_index()\n",
    "else:\n",
    "    # インデックスを作成する\n",
    "    create_index()\n",
    "\n",
    "    # 引数--docsで指定されたディレクトリ内のファイルを読み込んで、Azure AI Searchにインデックスする\n",
    "    for filename in glob.glob(args.docs):\n",
    "        # ドキュメントからテキストを抽出する\n",
    "        content = extract_text_from_docs(filename)\n",
    "\n",
    "        # テキストを指定したサイズで分割する\n",
    "        chunksize = int(args.chunksize)\n",
    "        overlap = int(args.overlap)\n",
    "        result = create_chunk(content, separator, chunksize, overlap)\n",
    "\n",
    "        # テキストをAzure AI Searchにインデックスする\n",
    "        index_docs(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
